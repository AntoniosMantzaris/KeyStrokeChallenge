Data exploration (At the beginning, I am doing some data exploration to get some initial information out of it.)
-Initially, I thought that the times corresponded to the duration of each key's pressing and releasing, influenced by an overlap I noticed between a release and the next press. The tests I conducted, indicated that the maximum time spent for pressing or releasing is always on the last key and the last iteration. These outcomes made me reject my initial assumption and continue with my second thought in which the times are timestamps of a stopwatch which started the moment of the first button is pressed. Further research would be conducted normally, including consultancy from the domain exprert's and data owner's side.
-Then I introduced the duration of a key pressing being the difference between release and press of the respective key, but also the delay which is the difference between two consecitive key presses. Some experimentation was followed on them and their maximum, mean per user/iteration to see if we can extract any information of these attributes (e.g., if every user performs similarly then the features are not insightful).

Feature Engineering (Create more meaningful features. Instead of raw timestamps, have duration of pressing a button (release-press), delay between two consecutive button presses (press(i+1)-press(i)), overlap (if someone presses the next key before they release the previous one) and some statistics of those indicating general behaviors of the users.)
-Data exploration showed that the users are responding differently on the introduced features and that is why these features were introduced to the training and testing dataset accompanied with some of their statistics which can indicate if a user is fast/slow, their performance is steady or fluctuates and their means as baseline.

Models
-The task is multi-class classification. The dataset is not that large for any model to be effective. Due to lack of time, just SVM and RandomForests were tried with different configurations such as on kernel technique for the SVM and cross-validation technique for both. RandomForests performed better, hence used. Further fine-tuning was performed both with grid and random search to present the best hyperparameters for the final model.

Results
-No ground-truth was given for the test dataset but the cross validation on the training dataset indicated accuracy around 77%. It is not the optimal, further testing on feature engineering part could help, other techniques such as normalization could also help, especially SVM, more hyperparameters' tuning could improve the performance as well and of course other models could be proven more appropriate. Both SVM and RandomForests do multiclass classification out-of-the-box but OneVsAll could also be tried instead of the OneVsOne. More data could enhance machine learning models and maybe make deep learnig techniques worth trying. However, considering the relativelly large amount of classes the produced accuracy is satisfactory.
There are a lot of steps that can be followed and produce a better prediction model. In this project, I tried to show the basic steps that I would follow in a similar task. As a general remark I would highlight that communication for the understanding of the data and the problem but also patience, persevearance and thorough research for the discovery of the most appropriate technique can be essential and hence should be always seeked.
